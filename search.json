[{"title":"用 RabbitMQ 的死信队列来做定时任务","url":"%2F2018%2F10%2F05%2F%E7%94%A8-RabbitMQ-%E7%9A%84%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97%E6%9D%A5%E5%81%9A%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F","content":"\n在开发中做定时任务是一个非常常见的业务场景，在代码层面 Node.js 可以用 setTimeout、setInerval 这种基础语法或用 [node-schedule](https://www.npmjs.com/package/node-schedule) 这些类似的库来达到部分目的，在第三方服务上可以用 Redis 的 Keyspace Notification 或 Linux 自身的 crontab 来做定时任务。RabbitMQ 作为一个消息中间件，使用其死信队列也可以达到做定时任务的目的。<!-- more -->\n\n![](http://nnblog-storage.b0.upaiyun.com/img/rabbitmq-1.png)\n\n本文以 Node.js 作为演示语言，操作 RabbitMQ 使用的是 [amqplib](https://www.npmjs.com/package/amqplib)。\n\n### 死信队列\nRabbitMQ 中有一种交换器叫 DLX，\b全称为 Dead-Letter-Exchange，可以称之为死信交换器。当消息在一个队列中变成死信（dead message）之后，它\b会被重新发送到另外一个交换器中，这个交换器就是 DLX，绑定在 DLX 上的队列就称之为死信队列。\n消息变成死信一般是以下几种情况：\n\n- 消息被拒绝，并且设置 requeue 参数为 false\n- 消息过期\n- 队列达到最大长度\n\nDLX 也是一个正常的交换器，和一般的交换器没有区别，它能在任何队列上被指定，实际上就是设置某个队列的属性。当这个队列存在死信时，RabbitMQ 就会自动地将这个消息重新发布到设置的 DLX 上去，进而被路由到另一个队列，即死信队列。要为某个队列添加 DLX，需要在创建这个队列的时候设置其`deadLetterExchange` 和 `deadLetterRoutingKey` 参数，`deadLetterRoutingKey` 参数可选，表示为 DLX 指定的路由键，如果没有特殊指定，则使用原队列的路由键。\n\n```javascript\nconst amqp = require('amqplib');\n\nconst myNormalEx = 'my_normal_exchange';\nconst myNormalQueue = 'my_normal_queue';\nconst myDeadLetterEx = 'my_dead_letter_exchange';\nconst myDeadLetterRoutingKey = 'my_dead_letter_routing_key';\nlet connection, channel;\namqp.connect('amqp://localhost')\n  .then((conn) => {\n    connection = conn;\n    return conn.createChannel();\n  })\n  .then((ch) => {\n    channel = ch;\n    ch.assertExchange(myNormalEx, 'direct', { durable: false });\n    return ch.assertQueue(myNormalQueue, {\n      exclusive: false,\n      deadLetterExchange: myDeadLetterEx,\n      deadLetterRoutingKey: myDeadLetterRoutingKey,\n    });\n  })\n  .then((ok) => {\n    channel.bindQueue(ok.queue, myNormalEx);\n    channel.sendToQueue(ok.queue, Buffer.from('hello'));\n    setTimeout(function () { connection.close(); process.exit(0) }, 500);\n  })\n  .catch(console.error);\n\n```\n\n上面的代码先声明了一个交换器 `myNormalEx`， 然后声明了一个队列 `myNormalQueue`，在声明该队列的时候通过设置其 `deadLetterExchange` 参数，为其添加了一个 DLX。所以当队列 `myNormalQueue` 中有消息成为死信后就会被发布到 `myDeadLetterEx` 中去。\n\n### 过期时间（TTL）\n\n在 RabbbitMQ 中，可以对消息和队列设置过期时间。当通过队列属性设置过期时间时，队列中所有消息都有相同的过期时间。当对消息设置单独的过期时间时，每条消息的 TTL 可以不同。如果两种方法一起使用，则消息的 TTL 以两者之间较小的那个数值为准。消息在队列中的生存时间一旦超过设置的 TTL 值时，就会变成“死信”（Dead Message），消费者将无法再接收到该消息。\n\n针对每条消息设置 TTL 是在发送消息的时候设置 `expiration` 参数，单位为毫秒。\n\n```javascript\nconst amqp = require('amqplib');\n\nconst myNormalEx = 'my_normal_exchange';\nconst myNormalQueue = 'my_normal_queue';\nconst myDeadLetterEx = 'my_dead_letter_exchange';\nconst myDeadLetterRoutingKey = 'my_dead_letter_routing_key';\nlet connection, channel;\namqp.connect('amqp://localhost')\n  .then((conn) => {\n    connection = conn;\n    return conn.createChannel();\n  })\n  .then((ch) => {\n    channel = ch;\n    ch.assertExchange(myNormalEx, 'direct', { durable: false });\n    return ch.assertQueue(myNormalQueue, {\n      exclusive: false,\n      deadLetterExchange: myDeadLetterEx,\n      deadLetterRoutingKey: myDeadLetterRoutingKey,\n    });\n})\n  .then((ok) => {\n    channel.bindQueue(ok.queue, myNormalEx);\n    channel.sendToQueue(ok.queue, Buffer.from('hello'), { expiration: '4000'});\n    setTimeout(function () { connection.close(); process.exit(0) }, 500);\n  })\n  .catch(console.error);\n\n```\n\n上面的代码在向队列发送消息的时候，通过传递 `{ expiration: '4000'}` 将这条消息的过期时间设为了4秒，对消息设置4秒钟过期，这条消息并不一定就会在4秒钟后被丢弃或进入死信，只有当这条消息到达队首即将被消费时才会判断其是否过期，若未过期就会被消费者消费，若已过期就会被删除或者成为死信。\n\n### 定时任务\n\n因为队列中的消息过期后会成为死信，而死信又会被发布到该消息所在的队列的 DLX 上去，**所以通过为消息设置过期时间，然后再消费该消息所在队列的 DLX 所绑定的队列，从而来达到定时处理一个任务的目的。** 简单的讲就是当有一个队列 queue1，其 DLX 为 deadEx1，deadEx1 绑定了一个队列 deadQueue1，当队列 queue1 中有一条消息因过期成为死信时，就会被发布到 deadEx1 中去，通过消费队列 deadQueue1 中的消息，也就相当于消费的是 queue1 中的因过期产生的死信消息。\n\n![](http://nnblog-storage.b0.upaiyun.com/img/dead-letter.png)\n\n消费死信队列的代码如下：\n\n```javascript\nconst amqp = require('amqplib');\n\nconst myDeadLetterEx = 'my_dead_letter_exchange';\nconst myDeadLetterQueue = 'my_dead_letter_queue';\nconst myDeadLetterRoutingKey = 'my_dead_letter_routing_key';\nlet channel;\namqp.connect('amqp://localhost')\n.then((conn) => {\n  return conn.createChannel();\n})\n.then((ch) => {\n  channel = ch;\n  ch.assertExchange(myDeadLetterEx, 'direct', { durable: false });\n  return ch.assertQueue(myDeadLetterQueue, { exclusive: false });\n})\n.then((ok) => {\n  channel.bindQueue(ok.queue, myDeadLetterEx, myDeadLetterRoutingKey);\n  channel.consume(ok.queue, (msg) => {\n    console.log(\" [x] %s: '%s'\", msg.fields.routingKey, msg.content.toString());\n  }, { noAck: true})\n})\n.catch(console.error);\n```\n\n这里需要注意的是，如果声明的 `myDeadLetterEx` 是 direct 类型，那么在为其绑定队列的时候一定要指定 BindingKey，即这里的 `myDeadLetterRoutingKey`，如果不指定 Bindingkey，则需要将 `myDeadLetterEx` 声明为 fanout 类型。","tags":["RabbitMQ"]},{"title":"让Express支持async/await","url":"%2F2017%2F10%2F07%2F%E8%AE%A9Express%E6%94%AF%E6%8C%81async-await%2F","content":"随着 Node.js v8 的发布，Node.js 已原生支持 async/await 函数，Web 框架 Koa 也随之发布了 Koa 2 正式版，支持 async/await 中间件，为处理异步回调带来了极大的方便。\n<!-- more -->\n既然 Koa 2 已经支持 async/await 中间件了，为什么不直接用 Koa，而还要去改造 Express 让其支持 async/await 中间件呢？因为 Koa 2 正式版发布才不久，而很多老项目用的都还是 Express，不可能将其推倒用 Koa 重写，这样成本太高，但又想用到新语法带来的便利，那就只能对 Express 进行改造了，而且这种改造必须是对业务无侵入的，不然会带来很多的麻烦。\n\n### 直接使用 async/await\n让我们先来看下在 Express 中直接使用 async/await 函数的情况。\n```javascript\nconst express = require('express');\nconst app = express();\nconst { promisify } = require('util');\nconst { readFile } = require('fs');\nconst readFileAsync = promisify(readFile);\n   \napp.get('/', async function (req, res, next) {\n  const data = await readFileAsync('./package.json');\n  res.send(data.toString());\n});\n// Error Handler\napp.use(function (err, req, res, next) {\n  console.error('Error:', err);\n  res.status(500).send('Service Error');\n});\n   \napp.listen(3000, '127.0.0.1', function () {\n  console.log(`Server running at http://${ this.address().address }:${ this.address().port }/`);\n});\n```\n上面是没有对 Express 进行改造，直接使用 async/await 函数来处理请求，当请求`http://127.0.0.1:3000/`时，发现请求能正常请求，响应也能正常响应。这样似乎不对 Express 做任何改造也能直接使用 async/await 函数，但如果 async/await 函数里发生了错误能不能被我们的错误处理中间件处理呢？现在我们去读取一个不存在文件，例如将之前读取的`package.json`换成`age.json`。\n```javascript\napp.get('/', async function (req, res, next) {\n  const data = await readFileAsync('./age.json');\n  res.send(data.toString());\n});\n```\n现在我们去请求`http://127.0.0.1:3000/`时，发现请求迟迟不能响应，最终会超时。而在终端报了如下的错误：\n![UnhandlerRejectionError](http://nnblog-storage.b0.upaiyun.com/img/unhandlererror.png)\n发现错误并没有被错误处理中间件处理，而是抛出了一个`unhandledRejection`异常，现在如果我们用 try/catch 来手动捕获错误会是什么情况呢？\n```javascript\napp.get('/', async function (req, res, next) {\n  try {\n    const data = await readFileAsync('./age.json');\n    res.send(datas.toString());\n  } catch(e) {\n    next(e);\n  }\n});\n```\n发现请求被错误处理中间件处理了，说明我们手动显式的来捕获错误是可以的，但是如果在每个中间件或请求处理函数里面加一个 try/catch 也太不优雅了，对业务代码有一定的侵入性，代码也显得难看。所以通过直接使用 async/await 函数的实验，我们发现对 Express 改造的方向就是能够接收 async/await 函数里面抛出的错误，又对业务代码没有侵入性。\n\n### 改造 Express\n在 Express 中有两种方式来处理路由和中间件，一种是通过 Express 创建的 app，直接在 app 上添加中间件和处理路由，像下面这样：\n```javascript\nconst express = require('express');\nconst app = express();\n   \napp.use(function (req, res, next) {\n  next();\n});\napp.get('/', function (req, res, next) {\n  res.send('hello, world');\n});\napp.post('/', function (req, res, next) {\n  res.send('hello, world');\n});\n   \napp.listen(3000, '127.0.0.1', function () {\n  console.log(`Server running at http://${ this.address().address }:${ this.address().port }/`);\n});\n```\n另外一种是通过 Express 的 Router 创建的路由实例，直接在路由实例上添加中间件和处理路由，像下面这样：\n```javascript\nconst express = require('express');\nconst app = express();\nconst router = new express.Router();\napp.use(router);\n   \nrouter.get('/', function (req, res, next) {\n  res.send('hello, world');\n});\nrouter.post('/', function (req, res, next) {\n  res.send('hello, world');\n});\n   \napp.listen(3000, '127.0.0.1', function () {\n  console.log(`Server running at http://${ this.address().address }:${ this.address().port }/`);\n});\n```\n这两种方法可以混合起来用，现在我们思考一下怎样才能让一个形如`app.get('/', async function(req, res, next){})`的函数，让里面的 async 函数抛出的错误能被统一处理呢？要让错误被统一的处理当然要调用 `next(err)` 来让错误被传递到错误处理中间件，又由于 async 函数返回的是 Promise，所以肯定是形如这样的`asyncFn().then().catch(function(err){ next(err) })`，所以按这样改造一下就有如下的代码：\n```javascript\napp.get = function (...data) {\n  const params = [];\n  for (let item of data) {\n    if (Object.prototype.toString.call(item) !== '[object AsyncFunction]') {\n      params.push(item);\n      continue;\n    }\n    const handle = function (...data) {\n      const [ req, res, next ] = data;\n      item(req, res, next).then(next).catch(next);\n    };\n    params.push(handle);\n  }\n  app.get(...params)\n}\n```\n上面的这段代码中，我们判断`app.get()`这个函数的参数中，若有 async 函数，就采用`item(req, res, next).then(next).catch(next);`来处理，这样就能捕获函数内抛出的错误，并传到错误处理中间件里面去。但是这段代码有一个明显的错误就是最后调用 app.get()，这样就递归了，破坏了 app.get 的功能，也根本处理不了请求，因此还需要继续改造。\n我们之前说 Express 两种处理路由和中间件的方式可以混用，那么我们就混用这两种方式来避免递归，代码如下：\n```javascript\nconst express = require('express');\nconst app = express();\nconst router = new express.Router();\napp.use(router);\n    \napp.get = function (...data) {\n  const params = [];\n  for (let item of data) {\n    if (Object.prototype.toString.call(item) !== '[object AsyncFunction]') {\n      params.push(item);\n      continue;\n    }\n    const handle = function (...data) {\n      const [ req, res, next ] = data;\n      item(req, res, next).then(next).catch(next);\n    };\n    params.push(handle);\n  }\n  router.get(...params)\n}\n```\n像上面这样改造之后似乎一切都能正常工作了，能正常处理请求了。但通过查看 Express 的源码，发现这样破坏了 app.get() 这个方法，因为 app.get() 不仅能用来处理路由，而且还能用来获取应用的配置，在 Express 中对应的源码如下：\n```javascript\nmethods.forEach(function(method){\n  app[method] = function(path){\n    if (method === 'get' && arguments.length === 1) {\n      // app.get(setting)\n      return this.set(path);\n    }\n    \n    this.lazyrouter();\n    \n    var route = this._router.route(path);\n    route[method].apply(route, slice.call(arguments, 1));\n    return this;\n  };\n});\n```\n所以在改造时，我们也需要对 app.get 做特殊处理。在实际的应用中我们不仅有 get 请求，还有 post、put 和 delete 等请求，所以我们最终改造的代码如下：\n```javascript\nconst { promisify } = require('util');\nconst { readFile } = require('fs');\nconst readFileAsync = promisify(readFile);\nconst express = require('express');\nconst app = express();\nconst router = new express.Router();\nconst methods = [ 'get', 'post', 'put', 'delete' ];\napp.use(router);\n    \nfor (let method of methods) {\n  app[method] = function (...data) {\n    if (method === 'get' && data.length === 1) return app.set(data[0]);\n\n    const params = [];\n    for (let item of data) {\n      if (Object.prototype.toString.call(item) !== '[object AsyncFunction]') {\n        params.push(item);\n        continue;\n      }\n      const handle = function (...data) {\n        const [ req, res, next ] = data;\n        item(req, res, next).then(next).catch(next);\n      };\n      params.push(handle);\n    }\n    router[method](...params);\n  };\n}\n      \napp.get('/', async function (req, res, next) {\n  const data = await readFileAsync('./package.json');\n  res.send(data.toString());\n});\n      \napp.post('/', async function (req, res, next) {\n  const data = await readFileAsync('./age.json');\n  res.send(data.toString());\n});\n    \nrouter.use(function (err, req, res, next) {\n  console.error('Error:', err);\n  res.status(500).send('Service Error');\n}); \n     \napp.listen(3000, '127.0.0.1', function () {\n  console.log(`Server running at http://${ this.address().address }:${ this.address().port }/`);\n});\n```\n现在就改造完了，我们只需要加一小段代码，就可以直接用 async function 作为 handler 处理请求，对业务也毫无侵入性，抛出的错误也能传递到错误处理中间件。\n","tags":["Express"]},{"title":"让写入数据库的数据自动写入缓存","url":"%2F2017%2F03%2F24%2F%E8%AE%A9%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E8%87%AA%E5%8A%A8%E5%86%99%E5%85%A5%E7%BC%93%E5%AD%98%2F","content":"\n在项目开发中，为了减轻数据库的 I/O 压力，加快请求的响应速度，缓存是常用到的技术。[Redis](https://redis.io/) 和 [Memcache](https://memcached.org/) 是现在常用的两个用来做数据缓存的技术。\n<!-- more -->\n数据缓存一些常见的做法是，让数据写入到数据库以后通过一些自动化的脚本自动同步到缓存，或者在向数据库写数据后再手动向缓存写一次数据。这些做法不免都有些繁琐，且代码也不好维护。我在写 Node.js 项目的时候，发现**利用 [Mongoose](https://www.npmjs.com/package/mongoose)（一个 MongoDB 的 ODM）和 [Sequelize](https://www.npmjs.com/package/sequelize)（一个 MySQL 的 ORM）的一些功能特性能够优雅的做到让写入到 MongoDB/MySQL 的数据自动写入到 Redis，并且在做查询操作的时候能够自动地优先从缓存中查找数据，若缓存中找不到才进入 DB 中查找，并将 DB 中找到的数据写入缓存。**\n\n本文不讲解 Mongoose 和 Sequelize 的基本用法，这里只讲解如何做到上面所说的自动缓存。\n> 本文要用到的一些库为 [Mongoose](https://www.npmjs.com/package/mongoose)、[Sequelize](https://www.npmjs.com/package/sequelize)、[ioredis](https://www.npmjs.com/package/ioredis) 和 [lodash](https://www.npmjs.com/package/lodash)。Node.js 版本为 7.7.1。\n\n### 在 MongoDB 中实现自动缓存\n```javascript\n// redis.js\nconst Redis = require('ioredis');\nconst Config = require('../config');\n     \nconst redis = new Redis(Config.redis);\n     \nmodule.exports = redis;\n```\n上面文件的代码主要用于连接 redis。\n```javascript\n// mongodb.js\nconst mongoose = require('mongoose');\n    \nmongoose.Promise = global.Promise;\nconst demoDB = mongoose.createConnection('mongodb://127.0.0.1/demo', {});\n     \nmodule.exports = demoDB;\n```\n上面是连接 mongodb 的代码。\n\n```javascript\n// mongoBase.js\nconst mongoose = require('mongoose');\nconst Schema = mongoose.Schema;\nconst redis = require('./redis');\n      \nfunction baseFind(method, params, time) {\n  const self = this;\n  const collectionName = this.collection.name;\n  const dbName = this.db.name;\n  const redisKey = [dbName, collectionName, JSON.stringify(params)].join(':');\n  const expireTime = time || 3600;\n      \n  return new Promise(function (resolve, reject) {\n    redis.get(redisKey, function (err, data) {\n      if (err) return reject(err);\n      if (data) return resolve(JSON.parse(data));\n      \n      self[method](params).lean().exec(function (err, data) {\n        if (err) return reject(err);\n        if (Object.keys(data).length === 0) return resolve(data);\n      \n        redis.setex(redisKey, expireTime, JSON.stringify(data));\n        resolve(data);\n      });\n    });\n  });\n}\n      \nconst Methods = {\n  findCache(params, time) {\n    return baseFind.call(this, 'find', params, time);\n  },\n  findOneCache(params, time) {\n    return baseFind.call(this, 'findOne', params, time);\n  },\n  findByIdCache(params, time) {\n    return baseFind.call(this, 'findById', params, time);\n  },\n};\n     \nconst BaseSchema = function () {\n  this.defaultOpts = {\n  };\n};\n      \nBaseSchema.prototype.extend = function (schemaOpts) {\n  const schema = this.wrapMethods(new Schema(schemaOpts, {\n    toObject: { virtuals: true },\n    toJSON: { virtuals: true },\n  }));\n       \n  return schema;\n};\n\nBaseSchema.prototype.wrapMethods = function (schema) {\n  schema.post('save', function (data) {\n    const dbName = data.db.name;\n    const collectionName = data.collection.name;\n    const redisKey = [dbName, collectionName, JSON.stringify(data._id)].join(':');\n     \n    redis.setex(redisKey, 3600, JSON.stringify(this));\n  });\n     \n  Object.keys(Methods).forEach(function (method) {\n    schema.statics[method] = Methods[method];\n  });\n  return schema;\n};\n       \nmodule.exports = new BaseSchema();\n```\n上面的代码是在用 mongoose 建模的时候，所有的 schema 都会继承这个 BaseSchema。这个 BaseSchema 里面就为所有继承它的 schema 添加了一个模型在执行 save 方法后触发的文档中间件，这个中间件的作用就是在数据被写入 MongoDB 后再自动写入 redis。然后还为每个继承它的 schema 添加了三个静态方法，分别是 findByIdCache、findOneCache 和 findCache，它们分别是 findById、findOne 和 find 方法的扩展，只是不同之处在于用添加的三个方法进行查询时会根据传入的条件先从 redis 中查找数据，若查到就返回数据，若查不到就继续调用所对应的的原生方法进入 MongoDB 中查找，若在 MongoDB 中查到了，就把查到的数据写入 redis，供以后的查询使用。添加的这三个静态方法的调用方法和他们所对应的的原生方法一致，只是可以多传入一个时间，用来设置数据在缓存中的过期时间。\n```javascript\n// userModel.js\nconst BaseSchema = require('./mongoBase');\nconst mongoDB = require('./mongodb.js');\n     \nconst userSchema = BaseSchema.extend({\n  name: String,\n  age: Number,\n  addr: String,\n});\n       \nmodule.exports = mongoDB.model('User', userSchema, 'user');\n```\n这是为 user 集合建的一个模型，它就通过 BaseSchema.extend 方法继承了上面说到的中间件和静态方法。\n```javascript\n// index.js\nconst UserModel = require('./userModl');\n      \nconst action = async function () {\n  const user = await UserModel.create({ name: 'node', age: 7, addr: 'nodejs.org' });\n  const data1 = await UserModel.findByIdCache(user._id.toString());\n  const data2 = await UserModel.findOneCache({ age: 7 });\n  const data3 = await UserModel.findCache({ name: 'node', age: 7 }, 7200);\n  return [ data1, data2, data3];\n};\n       \naction().then(console.log).catch(console.error);\n```\n上面的的代码就是向 User 集合中写了一条数据后，然后依次调用了我们自己添加的三个用于查询的静态方法。把 redis 的 monitor 打开，发现代码已经按我们预想的那样执行了。\n\n总结，上面的方案主要通过 Mongoose 的中间件和静态方法达到了我们想要的功能。但添加的 findOneCache 和 findCache 方法很难达到较高的数据一致性，若要追求很强的数据一致性就用它们所对应的的 findOne 和 find。findByIdCache 能保证很好的数据一致性，但也仅限于修改数据时是查询出来再 save，若是直接 update 也做不到数据一致性。\n### 在 MySQL 中实现自动缓存\n```javascript\n// mysql.js\nconst Sequelize = require('sequelize');\nconst _ = require('lodash');\nconst redis = require('./redis');\n    \nconst setCache = function (data) {\n  if (_.isEmpty(data) || !data.id) return;\n      \n  const dbName = data.$modelOptions.sequelize.config.database;\n  const tableName = data.$modelOptions.tableName;\n  const redisKey = [dbName, tableName, JSON.stringify(data.id)].join(':')\n  redis.setex(redisKey, 3600, JSON.stringify(data.toJSON()));\n};\n      \nconst sequelize = new Sequelize('demo', 'root', '', {\n  host: 'localhost',\n  port: 3306,\n  hooks: {\n    afterUpdate(data) {\n      setCache(data);\n    },\n    afterCreate(data) {\n      setCache(data);\n    },\n  },\n});\n      \nsequelize\n  .authenticate()\n  .then(function () {\n    console.log('Connection has been established successfully.');\n  })\n  .catch(function (err) {\n    console.error('Unable to connect to the database:', err);\n  });\n      \nmodule.exports = sequelize;\n```\n上面的代码主要作用是连接 MySQL 并生成 sequelize 实例，在构建 sequelize 实例的时候添加了两个钩子方法 afterUpdate 和 afterCreate。afterUpdate 用于在模型实例更新后执行的函数，注意**必须是模型实例更新**才会触发此方法，如果是直接类似 Model.update 这种方式更新是不会触发这个钩子函数的，只能是一个已经存在的实例调用 save 方法的时候会触发这个钩子。afterCreate 是在模型实例创建后调用的钩子函数。这两个钩子的主要目的就是用来当一条数据被写入到 MySQL 后，再自动的写入到 redis，即实现自动缓存。\n```javascript\n// mysqlBase.js\nconst _ = require('lodash');\nconst Sequelize = require('sequelize');\nconst redis = require('./redis');\n       \nfunction baseFind(method, params, time) {\n  const self = this;\n  const dbName = this.sequelize.config.database;\n  const tableName = this.name;\n  const redisKey = [dbName, tableName, JSON.stringify(params)].join(':');\n  return (async function () {\n    const cacheData = await redis.get(redisKey);\n    if (!_.isEmpty(cacheData)) return JSON.parse(cacheData);\n       \n    const dbData = await self[method](params);\n    if (_.isEmpty(dbData)) return {};\n      \n    redis.setex(redisKey, time || 3600, JSON.stringify(dbData));\n    return dbData;\n  })();\n}\n       \nconst Base = function (sequelize) {\n  this.sequelize = sequelize;\n};\n         \nBase.prototype.define = function (model, attributes, options) {\n  const self = this;\n  return this.sequelize.define(model, _.assign({\n    id: {\n      type: Sequelize.UUID,\n      primaryKey: true,\n      defaultValue: Sequelize.UUIDV1,\n    },\n  }, attributes), _.defaultsDeep({\n    classMethods: {\n      findByIdCache(params, time) {\n        this.sequelize = self.sequelize;\n        return baseFind.call(this, 'findById', params, time);\n      },\n      findOneCache(params, time) {\n        this.sequelize = self.sequelize;\n        return baseFind.call(this, 'findOne', params, time);\n      },\n      findAllCache(params, time) {\n        this.sequelize = self.sequelize;\n        return baseFind.call(this, 'findAll', params, time);\n      },\n    },\n  }, options));\n};\n      \nmodule.exports = Base;\n```\n上面的代码同之前的 mongoBase 的作用大致一样。在 sequelize 建模的时候，所有的 schema 都会继承这个 Base。这个 Base 里面就为所有继承它的 schema 添加了三个静态方法，分别是 findByIdCache、findOneCahe 和 findAllCache，它们的作用和之前 mongoBase 中的那三个方法作用一样，只是为了和 sequelize 中原生的 findAll 保持一致，findCache 在这里变成了 findAllCache。在 sequelize 中为 schema 添加类方法（classMethods），即相当于在 mongoose 中为 schema 添加静态方法（statics)。\n```javascript\n// mysqlUser.js\nconst Sequelize = require('sequelize');\nconst base = require('./mysqlBase.js');\nconst sequelize = require('./mysql.js');\n       \nconst Base = new base(sequelize);\nmodule.exports = Base.define('user', {\n  name: Sequelize.STRING,\n  age: Sequelize.INTEGER,\n  addr: Sequelize.STRING,\n}, {\n  tableName: 'user',\n  timestamps: true,\n});\n```\n上面定义了一个 User schema，它就从 Base 中继承了findByIdCache、findOneCahe 和 findAllCache。\n```javascript\nconst UserModel = require('./mysqlUser');\n      \nconst action = async function () {\n  await UserModel.sync({ force: true });\n  const user = await UserModel.create({ name: 'node', age: 7, addr: 'nodejs.org' });\n  await UserModel.findByIdCache(user.id);\n  await UserModel.findOneCache({ where: { age: 7 }});\n  await UserModel.findAllCache({ where: { name: 'node', age: 7 }}, 7200);\n  return 'finish';\n};\n       \naction().then(console.log).catch(console.error);\n```\n总结，通过 sequelize 实现的自动缓存方案和之前 mongoose 实现的一样，也会存在数据一致性问题，findByIdCache 较好，findOneCache 和 findAllCache 较差，当然这里很多细节考虑得不够完善，可根据业务合理调整。\n","tags":["MySQL"]},{"title":"由left-pad扯到JS中的位运算","url":"%2F2017%2F01%2F07%2F%E7%94%B1left-pad%E6%89%AF%E5%88%B0JS%E4%B8%AD%E7%9A%84%E4%BD%8D%E8%BF%90%E7%AE%97%2F","content":"\n这个话题的由来是2016年3月份的时候 [NPM](https://www.npmjs.com/) 社区发生了‘left-pad’事件，不久后社区就有人发布了用来补救的，也是现在大家能用到的 [left-pad](https://www.npmjs.com/package/left-pad) 库。\n\n<!-- more -->\n\n最开始这个库的代码是这样的。\n\n```javascript\nmodule.exports = leftpad;\n          \nfunction leftpad (str, len, ch) {\n  str = String(str);\n  \n  var i = -1;\n  \n  if (!ch && ch !== 0) ch = ' ';\n  \n  len = len - str.length;\n  \n  while (++i < len) {\n    str = ch + str;\n  }\n  \n  return str;\n}\n```\n\n我第一次看到这段代码的时候，没看出什么毛病，觉得清晰明了。后来刷微博的时候[@左耳朵耗子](http://weibo.com/p/1005051401880315/home?from=page_100505&mod=TAB#place)老师指出了这段代码可以写得更有效率些，于是他就贴出了自己写的版本并给 left-pad 提了 PR，代码如下：\n\n```javascript\nmodule.exports = leftpad;\n     \nfunction leftpad (str, len, ch) {\n  //convert the `str` to String\n  str = str +'';\n     \n  //needn't to pad\n  len = len - str.length;\n  if (len <= 0) return str;\n     \n  //convert the `ch` to String\n  if (!ch && ch !== 0) ch = ' ';\n  ch = ch + '';\n     \n  var pad = '';\n  while (true) {\n    if (len & 1) pad += ch;\n    len >>= 1;\n    if (len) ch += ch;\n    else break;\n  }\n  return pad + str;\n}\n```\n\n我当时看到他的这段代码的里面的 `&`和`>>`运算符的时候一下有点懵了，只知道这是位运算里面的‘按位与’和‘右移’运算，但是完全不知道为什么这样写就能提高效率。于是就想着去了解位运算的实质和使用场景。\n\n在了解位运算之前，我们先必须了解一下什么是原码、反码和补码以及二进制与十进制的转换。\n\n### 原码、补码和反码\n\n#### 原码\n一个数在计算机中是以二进制的形式存在的，其中第一位存放符号, 正数为0, 负数为1。原码就是用第一位存放符号的二进制数值。例如2的原码为00000010，-2的原码为10000010。\n\n#### 反码\n正数的反码是其本身。负数的反码是在其原码的基础上，符号位不变，其余各位取反，即0变1，1变0。\n```javascript\n[+3]=[00000011]原=[00000011]反\n[-3]=[10000011]原=[11111100]反\n```\n可见如果一个反码表示的是负数，并不能直观的看出它的数值，通常要将其转换成原码再计算。\n\n#### 补码\n正数的补码是其本身。负数的补码是在其原码的基础上，符号位不变，其余各位取反，最后+1。（即负数的补码为在其反码的基础上+1）。\n```javascript\n[+3]=[00000011]原=[00000011]反=[00000011]补\n[-3]=[10000011]原=[11111100]反=[11111101]补\n```\n可见对于负数，补码的表示方式也是让人无法直观看出其数值的，通常也需要转换成原码再计算。\n\n### 二进制与十进制的转换\n二进制与十进制的区别在于数运算时是逢几进一位。二进制是逢2进一位，十进制也就是我们常用的0-9是逢10进一位\n\n#### 正整数的十进制转二进制\n正整数的十进制转二进制的方法为将一个十进制数除以2，得到的商再除以2，以此类推直到商等于1或0时为止，倒取除得的余数，即为转换所得的二进制数的结果。\n\n例如把52换算成二进制数，计算过程如下图：\n![10to2](http://nnblog-storage.b0.upaiyun.com/img/2to10.gif)\n52除以2得到的余数依次为：0、0、1、0、1、1，倒序排列，所以52对应的二进制数就是110100。\n\n#### 负整数的十进制转二进制\n负整数的十进制转二进制为将该负整数对应的正整数先转换成二进制，然后对其“取反”，再对取反后的结果+1。即负整数采用其二进制补码的形式存储。\n至于负数为什么要用二进制补码的形式存储，可参考一篇阮一峰的文章《[关于2的补码](http://www.ruanyifeng.com/blog/2009/08/twos_complement.html)》。\n例如 -52 的原码为 10110100，其反码为 11001011，其补码为 11001100。所以 -52 转换为二进制后为 11001100。\n\n#### 十进制小数转二进制\n十进制小数转二进制的方法为“乘2取整”，对十进制小数乘2得到的整数部分和小数部分，整数部分即是相应的二进制数码，再用2乘小数部分(之前乘后得到新的小数部分)，又得到整数和小数部分。\n如此不断重复，直到小数部分为0或达到精度要求为止。第一次所得到为最高位，最后一次得到为最低位。\n```\n如:0.25的二进制\n0.25*2=0.5 取整是0\n0.5*2=1.0    取整是1\n即0.25的二进制为 0.01 ( 第一次所得到为最高位,最后一次得到为最低位)\n   \n0.8125的二进制\n0.8125*2=1.625   取整是1\n0.625*2=1.25     取整是1\n0.25*2=0.5       取整是0\n0.5*2=1.0        取整是1\n即0.8125的二进制是0.1101\n```\n\n#### 二进制转十进制\n从最后一位开始算，依次列为第0、1、2...位，第n位的数（0或1）乘以2的n次方，将得到的结果相加就是得到的十进制数。\n例如二进制为110的数，将其转为十进制的过程如下\n![2to10](http://nnblog-storage.b0.upaiyun.com/img/2to10.png)\n个位数 0 与 2º 相乘：0 × 2º = 0\n十位数 1 与 2¹ 相乘：1 × 2¹ = 2\n百位数 1 与 2² 相乘：1 × 2² = 4\n将得到的结果相加：0+2+4=6\n所以二进制 110 转换为十进制后的数值为 6。\n\n小数二进制用数值乘以2的负幂次然后相加。\n\n### JavaScript 中的位运算\n在 ECMAScript 中按位操作符会将其操作数转成**补码形式的有符号32位整数。**下面是[ECMAScript 规格](http://www.ecma-international.org/ecma-262/6.0/#sec-binary-bitwise-operators-runtime-semantics-evaluation)中对于位运算的执行过程的表述：\n```\nThe production A : A @ B, where @ is one of the bitwise operators in the productions above, is evaluated as follows:\n1. Let lref be the result of evaluating A.\n2. Let lval be GetValue(lref).\n3. ReturnIfAbrupt(lval).\n4. Let rref be the result of evaluating B.\n5. Let rval be GetValue(rref).\n6. ReturnIfAbrupt(rval).\n7. Let lnum be ToInt32(lval).\n8. ReturnIfAbrupt(lnum).\n9. Let rnum be ToInt32(rval).\n10. ReturnIfAbrupt(rnum).\n11. Return the result of applying the bitwise operator @ to lnum and rnum. The result is a signed 32 bit integer.\n```\n需要注意的是第七步和第九步，根据 ES 的标准，超过32位的整数会被截断，而小数部分则会被直接舍弃。所以由此可以知道，在 JS 中，当位运算中有操作数大于或等于2³²时，就会出现意想不到的结果。\n\nJavaScript 中的位运算有：`&（按位与）`、`|（按位或）`、`~（取反）`、`^（按位异或）`、`<<（左移）`、`>>（有符号右移）`和`>>>（无符号右移）`。\n\n#### &按位与\n对每一个比特位执行与（AND）操作。只有 a 和 b 都是 1 时，a & b 才是 1。\n例如：9(base 10) & 14(base 10) = 1001(base2) & 1110(base 2) = 1000(base 2) = 8(base 10)\n\n因为当只有 a 和 b 都是 1 时，a&b才等于1，所以任一数值 x 与0（二进制的每一位都是0）按位与操作，其结果都为0。将任一数值 x 与 -1（二进制的每一位都是1）按位与操作，其结果都为 x。\n利用 & 运算的特点，我们可以用以简单的判断奇偶数，公式：\n```javascript\n(n & 1) === 0 //true 为偶数，false 为奇数。\n```\n因为 1 的二进制只有最后一位为1，其余位都是0，所以其判断奇偶的实质是判断二进制数最后一位是 0 还是 1。奇数的二进制最后一位是 1，偶数是0。\n\n当然还可以利用 JS 在做位运算时会舍弃掉小数部分的特性来做向下取整的运算，因为当 x 为整数时有 `x&-1=x`，所以当 x 为小数时有 `x&-1===Math.floor(x)`。\n\n#### |按位或\n对每一个比特位执行或（OR）操作。如果 a 或 b 为 1，则 a | b 结果为 1。\n例如：9(base 10) | 14(base 10) = 1001(base2) | 1110(base 2) = 1111(base 2) = 15(base 10)\n\n因为只要 a 或 b 其中一个是 1 时，a|b就等于1，所以任一数值 x 与-1（二进制的每一位都是1）按位与操作，其结果都为-1。将任一数值 x 与 0（二进制的每一位都是0）按位与操作，其结果都为 x。\n\n同样，按位或也可以做向下取整运算，因为当 x 为整数时有 `x|0=x`，所以当 x 为小数时有 `x|0===Math.floor(x)`。\n\n#### ~取反\n对每一个比特位执行非（NOT）操作。~a 结果为 a 的反转（即反码）。\n```\n9 (base 10)  = 00000000000000000000000000001001 (base 2)\n               --------------------------------\n~9 (base 10) = 11111111111111111111111111110110 (base 2) = -10 (base 10)\n```\n负数的二进制转化为十进制的规则是，符号位不变，其他位取反后加 1。\n\n对任一数值 x 进行按位非操作的结果为 -(x + 1)。~~x === x。\n\n同样，取反也可以做向下取整运算，因为当 x 为整数时有 `~~x===x`，所以当 x 为小数时有 `~~x===Math.floor(x)`。\n\n#### ^按位异或\n对每一对比特位执行异或（XOR）操作。当 a 和 b 不相同时，a ^ b 的结果为 1。\n例如：9(base 10) ^ 14(base 10) = 1001(base2) ^ 1110(base 2) = 0111(base 2) = 7(base 10)\n\n将任一数值 x 与 0 进行异或操作，其结果为 x。将任一数值 x 与 -1 进行异或操作，其结果为 ~x，即 x^-1=~x。\n同样，按位异或也可以做向下取整运算，因为当 x 为整数时有 `(x^0)===x`，所以当 x 为小数时有 `(x^0)===Math.floor(x)`。\n\n#### <<左移运算\n它把数字中的所有数位向左移动指定的数量，向左被移出的位被丢弃，右侧用 0 补充。\n例如，把数字 2（等于二进制中的 10）左移 5 位，结果为 64（等于二进制中的 1000000）：\n```javascript\nvar iOld = 2;\t\t//等于二进制 10\nvar iNew = iOld << 5;\t//等于二进制 1000000 十进制 64\n```\n因为二进制10转换成十进制的过程为 1×2¹+0×2º，在运算中2的指数与位置数相对应，当左移五位后就变成了 1×2¹⁺⁵+0×2º⁺⁵= 1×2¹×2⁵+0×2º×2⁵ = (1×2¹+0×2º)×2⁵。所以由此可以看出当2左移五位就变成了 2×2⁵=64。\n所以有一个数左移 n 为，即为这个数乘以2的n次方。`x<<n === x*2ⁿ`。\n同样，左移运算也可以做向下取整运算，因为当 x 为整数时有 `(x<<0)===x`，所以当 x 为小数时有 `(x<<0)===Math.floor(x)`。\n\n#### >>有符号右移运算\n它把 32 位数字中的所有数位整体右移，同时保留该数的符号（正号或负号）。有符号右移运算符恰好与左移运算相反。例如，把 64 右移 5 位，将变为 2。\n因为有符号右移运算符与左移运算相反，所以有一个数左移 n 为，即为这个数除以2的n次方。`x<<n === x/2ⁿ`。\n同样，有符号右移运算也可以做向下取整运算，因为当 x 为整数时有 `(x>>0)===x`，所以当 x 为小数时有 `(x>>0)===Math.floor(x)`。\n\n#### >>>无符号右移运算\n它将无符号 32 位数的所有数位整体右移。对于正数，无符号右移运算的结果与有符号右移运算一样，而负数则被作为正数来处理。\n```javascript\n-9 (base 10): 11111111111111111111111111110111 (base 2)\n                    --------------------------------\n-9 >>> 2 (base 10): 00111111111111111111111111111101 (base 2) = 1073741821 (base 10)\n```\n根据无符号右移的正数右移与有符号右移运算一样，而负数的无符号右移一定为非负的特征，可以用来判断数字的正负，如下：\n```javascript\nfunction isPos(n) {\n  return (n === (n >>> 0)) ? true : false;  \n}\n    \nisPos(-1); // false\nisPos(1); // true\n```\n\n### 总结\n根据 JS 的位运算，可以得出如下信息：\n1、所有的位运算都可以对小数取底。\n2、对于按位与`&`，可以用 `(n & 1) === 0 //true 为偶数，false 为奇数。`来判断奇偶。用`x&-1===Math.floor(x)`来向下取底。\n3、对于按位或`|`，可以用`x|0===Math.floor(x)`来向下取底。\n4、对于取反运算`~`，可以用`~~x===Math.floor(x)`来向下取底。\n5、对于异或运算`^`，可以用`(x^0)===Math.floor(x)`来向下取底。\n6、对于左移运算`<<`，可以`x<<n === x*2ⁿ`来求2的n次方，用`x<<0===Math.floor(x)`来向下取底。\n7、对于有符号右移运算`>>`，可以`x<<n === x/2ⁿ`求一个数字的 N 等分，用`x>>0===Math.floor(x)`来向下取底。\n8、对于无符号右移运算`>>>`，可以`(n === (n >>> 0)) ? true : false;`来判断数字正负，用`x>>>0===Math.floor(x)`来向下取底。\n\n用移位运算来替代普通算术能获得更高的效率。移位运算翻译成机器码的长度更短，执行更快，需要的硬件开销更小。\n","tags":["JavaScript"]},{"title":"通过单元测试为API自动生成文档","url":"%2F2017%2F01%2F03%2F%E9%80%9A%E8%BF%87%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E4%B8%BAAPI%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%96%87%E6%A1%A3%2F","content":"在开发中，为项目生成文档是很常见的需求，很多第三方库（如[jsdoc](https://www.npmjs.com/package/jsdoc)、[swagger](https://www.npmjs.com/package/swagger)等）的做法是为需要生成文档的函数编写相应的符合规范的注释，然后运行相应的命令，生成一个静态网页形式的文档。\n<!-- more -->\n\n用注释生成文档的好处是可以为无论是普通函数还是 API，只要编写了相应的注释都能生成相应的文档，然而这种做法总觉得有点繁琐，尤其是只需要为 API 生成文档的时候，需要手动编写大量的输入和输出作为使用示例。而且我只想需要 markdown 形式的文档，丢在内部 Gitlab 的 wiki 上供前端人员査阅，然后可以根据 commit 的 history 查阅不同的版本。\n\n不想手动为 API 文档编写大量的输入输出，那哪里会有输入输出呢，就很容易的想到了单元测试会产生输入和输出。好，那就用单元测试来为 API 生成文档。\n\n我在单元测试中主要用的库有 [mocha](https://www.npmjs.com/package/mocha)、[supertest](https://www.npmjs.com/package/supertest)和[power-assert](https://www.npmjs.com/package/power-assert)，Web 框架为 [express](https://www.npmjs.com/package/express)。\n\n\n完整代码示例：\n```javascript\n// app.js\nconst express = require('express');\nconst bodyParser = require('body-parser');\nconst app = express();\n     \napp.use(bodyParser.json());\napp.use(bodyParser.urlencoded({ extended: true }));\n    \napp.post('/user/create', function (req, res) {\n  const name = req.body.name;\n  res.json({\n    status: 200,\n    error_code: 0,\n    message: 'success',\n    data: {\n      id: '123abc',\n      name: 'node',\n      gender: 'male',\n      age: 23,\n    },\n  });\n});\n     \napp.get('/user/search', function (req, res) {\n  const name = req.query.name;\n  res.json({\n    status: 200,\n    error_code: 0,\n    message: 'success',\n    data: {\n      id: '123abc',\n      name: 'node',\n      gender: 'male',\n      age: 23,\n    },\n  });\n});\n     \napp.get('/user/:id', function (req, res) {\n  const userId = req.params.id;\n  res.json({\n    status: 200,\n    error_code: 0,\n    message: 'success',\n    data: {\n      id: '123abc',\n      name: 'node',\n      gender: 'male',\n      age: 23,\n    },\n  });\n});\n     \napp.listen(3000, function () {\n  console.log(`Server is listening on 3000`);\n});\nmodule.exports = app;\n```\n下面是测试文件。\n```javascript\n// test/uset.test.js\nconst assert = require('power-assert');\nconst supertest = require('supertest');\nconst U = require('../utils');\nconst app = require('../app');\nconst agent = supertest.agent(app);\n      \ndescribe('Test', function () {\n  it('should create user success', function (done) {\n    U.test({\n      agent,\n      file: 'user',\n      group: '用户相关API',\n      title: '创建用户',\n      method: 'post',\n      url: '/user/create'\n      params: {\n        name: { value: 'node', type: 'String', required: true, desc: '名称' },\n        gender: { value: 'male', type: 'String', required: false, desc: '性别' },\n        age: { value: 23, type: 'Int', required: false, desc: '' },\n      },\n      headers: { entrance: 'client' },\n      expect: 200,\n      callback (err, res) {\n        if (err) return done(err);\n\n        assert(res.body.data.name === 'node');\n        assert(res.body.data.age === 23);\n        done();\n      },\n    });\n  });\n      \n  it('should search user success', function (done) {\n    U.test({\n      agent,\n      file: 'user',\n      group: '用户相关API',\n      title: '搜索用户',\n      method: 'get',\n      url: '/user/search'\n      params: {\n        name: { value: 'node', type: 'String', required: true, desc: '名称' },\n      },\n      expect: 200,\n      callback (err, res) {\n        if (err) return done(err);\n     \n        assert(res.body.data.name === 'node');\n        assert(res.body.data.age === 23);\n        done();\n      },\n    });\n  });\n     \n  it('should search user success', function (done) {\n    U.test({\n      agent,\n      file: 'user',\n      group: '用户相关API',\n      title: '获取用户信息',\n      method: 'get',\n      url: '/user/:id'\n      params: {\n        id: { value: '123abc', type: 'String', required: true, desc: '' },\n      },\n      expect: 200,\n      callback (err, res) {\n        if (err) return done(err);\n        \n        assert(res.body.data.name === 'node');\n        assert(res.body.data.age === 23);\n        done();\n      },\n    });\n  });\n});\n```\n在测试文件中，测试用例的代码调用到了 utils.js 中的 test 方法，该方法的主要作用是接收单元测试的输入和输出并生成相应的文档，其中需要向 test 方法传入一个对象作为参数，对象中的字段解读如下：\n> agent：调用 API 的代理。\n> file：生成的文档的文件名称。\n> group：某一组文档的名称。\n> title：接口的名称。\n> method：接口的方法。\n> params：接口的参数，即输入。\n> headers: 添加到请求头中的信息。\n> expect：supertest 的expect。\n> callback：supertest 方法的 end 的回调函数。\n\nutils.js代码如下：\n```javascript\n// utils.js\nconst path = require('path');\nconst fs = require('fs');\n      \nconst mdStr = {};\nexports.test = function (obj) {\n  if (!mdStr[obj.group]) {\n    mdStr[obj.group] = '';\n    mdStr[obj.group] += '## ' + obj.group + '\\n\\n';\n  }\n  const fields = {};\n       \n  mdStr[obj.group] += `### ${ obj.title } \\`${ obj.method }\\` ${ obj.url } \\n\\n#### 参数\\n`;\n  mdStr[obj.group] += '\\n参数名 | 类型 | 是否必填 | 说明\\n-----|-----|-----|-----\\n';\n  Object.keys(obj.params).forEach(function (param) {\n    const paramVal = obj.params[param];\n    fields[param] = paramVal['value'];\n    mdStr[obj.group] += `${ param } | ${ paramVal['type'] } | ${ paramVal['required'] ? '是' : '否' } | ${ paramVal['desc'] } \\n`;\n  });\n  mdStr[obj.group] += '\\n#### 使用示例\\n\\n请求参数: \\n\\n';\n      \n  mdStr[obj.group] += '```json\\n' + JSON.stringify(fields, null, 2) + '\\n```\\n';\n  mdStr[obj.group] += '\\n返回结果:\\n\\n';\n       \n  if (obj.url.indexOf(':') > -1) {\n    obj.url = obj.url.replace(/:\\w*/g, function (word) {\n      return fields[word.substr(1)];\n    });\n  }\n        \n  obj.agent[obj.method](obj.url)\n  .set(obj.header || {})\n  .query(fields)\n  .send(fields)\n  .expect(obj.expect)\n  .end(function (err, res) {\n    mdStr[obj.group] += '```json\\n' + JSON.stringify(res.body, null, 2) + '\\n```\\n';\n    mdStr[obj.group] += '\\n';\n       \n    if (process.env['GEN_DOC'] > 0) {\n      fs.writeFileSync(path.resolve(__dirname, './docs/', obj.file + '.md'), mdStr[obj.group]);\n    }\n    obj.callback(err, res);\n  });\n}\n```\n这样，在根目录创建一个 docs 目录，运行 `npm run test:doc` 命令，就会在 docs 目录下生成文档。如果运行单元测试不想生成文档，直接用`npm test`就可以了，相应的package.json配置如下：\n```json\n\"scripts\": {\n  \"test\": \"export NODE_ENV='test' && mocha\",\n  \"test:doc\": \"export NODE_ENV='test' && export GEN_DOC=1 && mocha\"\n}\n```\n如果不想为某个 API 生成文档，就不要调用 utils 的 test，直接按原生的写法就可以了。\n\n若需要对参数进行签名，可在调用 test 方法时，增加形如`sign: true`的配置，然后在 test 方法中做相应的判断和实现相应的签名。\n生成的文档内容形式如下：\n![test_doc](http://nnblog-storage.b0.upaiyun.com/img/test_doc.jpg!watermark1.0)\n","tags":["doc"]},{"title":"利用redis将log4js产生的日志送到logstash","url":"%2F2016%2F08%2F23%2F%E5%88%A9%E7%94%A8redis%E5%B0%86log4js%E4%BA%A7%E7%94%9F%E7%9A%84%E6%97%A5%E5%BF%97%E9%80%81%E5%88%B0logstash%2F","content":"![log4js-logstash-redis](http://nnblog-storage.b0.upaiyun.com/img/log4js-logstash-redis.png!watermark1.0)\n\n在用Node.js开发项目的时候，我们常用 [log4js](https://www.npmjs.com/package/log4js) 模块来进行日志的记录，可以通过配置 log4js 的 [Appenders](https://github.com/nomiddlename/log4js-node/wiki/Appenders) 将日志输出到Console、File和GELF等不同的地方。\n<!-- more -->\n## logstash\nlogstash 是 [elastic](https://www.elastic.co/) 技术栈的其中一员，常被用来收集和解析日志。一种比较常见的做法就是在项目工程中把日志写入到日志文件中，然后用 logstash 读取和解析日志文件。比如在 Node.js 项目中，若要将日志记录到文件中，只需对 log4js 做如下配置即可：\n```javascript\nconst log4js = require('log4js');\nlog4js.configure({\n  appenders: [{\n    type: 'file',\n    filename: './example.log'\n  }]\n});\n        \nconst logger = log4js.getLogger();\nlogger.info('hello');\n```\n这样日志就会被记录到 `example.log` 文件中，然后再对 logstash 的 input 做如下配置：\n```\nfile {\n    path => \"YourLogPath/example.log\"\n    start_position => \"beginning\"\n  }\n}\n```\n这样 logstash 就可以读取这个日志文件。可是这样总感觉用一个文件作为中转略显麻烦，如果能将 log4js 产生的日志直接送到 logstash 就更好了。\n## logstashUDP\nlog4js 内置了 logstashUDP 来直接将日志输出到 logstash。配置如下：\n```javascript\nlog4js.configure({\n  appenders: [{\n    type: \"logstashUDP\",\n    host: \"localhost\",\n    port: 12345\n\t}]\n});\n```\n然后将 logstash 的配置改成下面这样：\n```\ninput {\n\tudp {\n    host => \"127.0.0.1\"\n\t\tport => 12345\n\t}\n}\n```\n嗯，很简单嘛！现在 log4js 产生的日志就能直接送到 logstash 了，而不再需要用一个文件作为中转。但是，当我在使用的时候发现一个问题，就是如果 logstash 服务挂了，这时候 log4js 仍然在不断的产生日志数据，这时候首先想到的就是把 logstash 重新启动起来，但重启后却发现 logstash 没能获取到在 logstash 挂掉的时候 log4js 产生的数据，也就是说如果 logstash 挂掉了的话，那么 log4js 产生的数据就会丢失，不会被处理。<br />\n这时候就想着用一个代理来暂存 log4js 产生的数据，log4js 将数据输出到代理，logstash 从代理那里读取数据，logstash 读取一条数据，代理就丢弃掉那条数据。对，也就是队列。这样就不会有数据丢失的问题了。\n## log4js-logstash-redis\n[log4js-logstash-redis](https://www.npmjs.com/package/log4js-logstash-redis) 就是为了解决上述问题而开发的一个 log4js 的 Appender。<br />\n他的原理是：log4js 将产生的日志输出到 redis，然后让 logstash 从 redis 读取数据。但是直接叫 log4js-redis 就好了嘛，为什么叫 log4js-logstash-redis 呢？这是因为将日志格式针对 logstash 做过一些更友好的定制。😊 <br />\n### 安装\n```\nnpm install log4js-logstash-redis --save\n```\n### 使用\n```javascript\nlog4js.configure({\n  appenders: [{\n    type: \"log4js-logstash-redis\",\n    key: \"test\",\n    redis: {\n      db: 1\n    }\n\t}]\n});\n```\nredis 配置可选，没有的话 redis 连接就采用默认值。logstash 的配置如下：\n```\nredis {\n\t\tdata_type => \"list\"\n\t\tkey => \"test\"\n\t\tcodec => json\n}\n```\n其中 data_type 的值一定要是 list。\n","tags":["logstash"]},{"title":"用ELK处理日志时碰到的几个问题","url":"%2F2016%2F08%2F16%2F%E7%94%A8ELK%E5%A4%84%E7%90%86%E6%97%A5%E5%BF%97%E6%97%B6%E7%A2%B0%E5%88%B0%E7%9A%84%E5%87%A0%E4%B8%AA%E9%97%AE%E9%A2%98%2F","content":"![elk](http://nnblog-storage.b0.upaiyun.com/img/elk.png)\n常说的ELK是指包括 elasticsearch、logstash 和 kibana 的一套技术栈，常用来处理日志等数据。最近在用这一套 stack 处理 nginx 的日志，而且还用到了 elastic 的另一个产品 beats，来作为客户端 shipper，形成了ELKB Stack。\n<!-- more -->\n<br />\n在使用这一套技术栈的时候碰到了一些其实并不高深的小问题，在这里写下来作为一个记录。\n\n## 日志格式\n常见的 nginx 的 access_log 日志都是用竖线`|`作为分隔，但我们的是用逗号`,`作为分隔，就像下面这样：\n```\n04/Aug/2016:15:37:09 +0800,112.124.127.44,\"-\",\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\",GET,/,0.003,0.003,146,199,23025,1,200,200,14,\"-\",TLSv1,AES256-SHA,127.0.0.1:8384\n```\n一开始我并没意识到这会存在什么问题，就在 logstash 的配置文件中用 filter 里的 ruby 来解析，如下：\n```\nruby {\n    init => \"@kname = ['time_local', 'remote_addr', 'remote_user', 'http_user_agent', 'request_method', 'uri', 'request_time', 'upstream_response_time', 'request_length', 'bytes_sent', 'connection', 'connection_request', 'status', 'upstream_status', 'body_bytes_sent', 'http_referer', 'ssl_protocol', 'ssl_cipher', 'upstream_addr']\"\n    code => \"\n      new_event = LogStash::Event.new(Hash[@kname.zip(event['message'].split(','))])\n      new_event.remove('@timestamp')\n      event.append(new_event)\n    \"\n    remove_field => [\"message\"]\n  }\n```\n无非就是把`split('|')`换成`split(',')`嘛，很简单！但是当我在终端观察输出的时候，发现却是这样的。\n![elk-1](http://nnblog-storage.b0.upaiyun.com/img/elk-1.png!watermark1.0)\nUser-Agent 被分割成了两个字段的内容，导致从`request_method`字段开始，字段名与其对应的值都错位了，原来在日志内容中的浏览器的User-Agent `\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\"`也包含逗号`,`导致它也被分割了，以致后面的内容都错位了，很明显这种情况不应该被分割。<br />\n由于对 ruby 不是很熟悉，就没有继续在ruby filter中想办法来解决这个问题，我就继续查阅了 Logstash 其他 filter 的用法，发现用 grok filter 似乎可以解决这个问题，于是就用 grok 替换了 ruby，配置如下：\n```\ngrok {\n        match=>{\"message\" => \"%{GREEDYDATA:time_local},%{IP:remote_addr},\\\"%{GREEDYDATA:remote_user}\\\",\\\"%{GREEDYDATA:http_user_agent}\\\",\n        %{WORD:request_method},%{URIPATHPARAM:uri},%{GREEDYDATA:request_time},%{GREEDYDATA:upstream_response_time},%{NUMBER:request_length},%{NUMBER:bytes_sent},\n        %{NUMBER:connection},%{NUMBER:connection_request},%{NUMBER:status},%{NUMBER:upstream_status},%{NUMBER:body_bytes_sent},\n        \\\"%{GREEDYDATA:http_referer}\\\",%{GREEDYDATA:ssl_protocol},%{GREEDYDATA:ssl_cipher},%{GREEDYDATA:upstream_addr}\"}\n    }\n```\n现在再重新解析观察一节结果。\n![elk-2](http://nnblog-storage.b0.upaiyun.com/img/elk-2.png!watermark1.0)\n嗯，问题解决，得到了正确的解析结果。但是，总觉得这样太不优雅了，看着无比的臃肿，而且这种正则匹配效率也不是很好。后来经同事提醒，csv filter 可以很好的解决这个问题，在查阅了csv filter的用法后，将 grok 替换成如下配置：\n```\ncsv {\n\t\tsource => \"message\"\n\t\tcolumns => [\"time_local\", \"remote_addr\", \"remote_user\", \"http_user_agent\", \"request_method\", \"uri\", \"request_time\", \"upstream_response_time\", \"request_length\", \"bytes_sent\", \"connection\", \"connection_request\", \"status\", \"upstream_status\", \"body_bytes_sent\", \"http_referer\", \"ssl_protocol\", \"ssl_cipher\", \"upstream_addr\"]\n\t\tconvert => {\n\t\t\t\"request_time\" => \"float\"\n\t\t\t\"upstream_response_time\" => \"float\"\n\t\t\t\"connection\" => \"integer\"\n\t\t\t\"connection_request\" => \"integer\"\n\t\t\t\"bytes_sent\" => \"integer\"\n\t\t\t\"body_bytes_sent\" => \"integer\"\n\t\t\t\"request_length\" => \"integer\"\n\t\t}\n\t}\n```\nOK，试了一下，完美解决，既优雅又正确。\n\n## geo_point\ngeo_point 是 elasticsearch 中的一种数据类型，Kibana 根据类型为 geo_point 的字段生成 Geo Coordinates。但是当我点Geo Coordinates得到的确实如下图的提示信息：\n![elk-3](http://nnblog-storage.b0.upaiyun.com/img/elk-3.png)\n在我的nginx-test1_log（nginx-test1_log为我用logstash将ngix日志导入到 elasticsearch 中取的索引名。）索引中不能找到类型geo_point类型的字段。<br />\nLogstash 中有一个geoip filter 是专门来处理将 ip 解析成地理坐标的，解析完后会得到location字段，我的配置如下：\n```\ngeoip {\n\t\tsource => remote_addr\n\t\tfields => [\"city_name\", \"country_name\", \"latitude\", \"longitude\", \"real_region_name\", \"location\"]\n\t\tremove_field => [\"[geoip][latitude]\", \"[geoip][longitude]\"]\n\t}\n```\nKibana 通常用那个location字段来生成 Geo Coordinates，于是我就去查看 nginx-test1_log 的mapping（通过 http://localhost:9200/nginx-test1_log/_mapping 查看。）发现location的类型是double，并不是geo_point，于是就想着把location字段的类型改为geo_point，然而发现并不能或者特别麻烦，就放弃了这个想法，Google 了一圈后，发现是 elasticsearch （以下简称『es』） 的 template 造成的（可通过 http://localhost:9200/_template 查看），es 会对 logstash 导入的数据采用默认的 logstash template，这个模板就通过 mapping 指定了location的类型为geo_point，但是如果在 Logstash 的 output 中为 es 指定的index的名字不是以logstash开头话，就不会采用这个模板，相应的location的类型就不会是geo_point了，由于我这里就是以nginx开头，所以就出现了这个问题。解决办法就是自己创建一个 template，然后在输出时指定，我直接把 logstash template 复制过来然后调整了下名字，调整后的 logstash 输出配置如下：\n```\noutput {\n  elasticsearch {\n      hosts => [\"0.0.0.0:9200\"]\n      index => \"nginx-%{source_tag}\"\n      template => \"/home/liuxin/logstash/templates/nginx-template.json\"\n      template_name => \"nginx-*\"\n      template_overwrite => true\n  }\n}\n```\n至此，问题就解决了。<br />\n入门ELK不久，感觉ELK高效、简单、易用和生态丰富，以后肯定还会碰到很多问题，再陆陆续续补充。\n","tags":["大数据"]},{"title":"利用Jenkins和Docker做持续集成","url":"%2F2016%2F07%2F13%2F%E5%88%A9%E7%94%A8Jenkins%E5%92%8CDocker%E5%81%9A%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%2F","content":"![docker_jenkins](http://nnblog-storage.b0.upaiyun.com/img/docker_jenkins.jpg!watermark1.0)\n持续集成（Continuous integration，简称CI）是一套标准的互联网软件开发和发布流程，主要指频繁的将代码集成到主干，让产品可以快速迭代，同时能保证高质量。\n<!-- more -->\n## 什么是Jenkins\nJenkins是一款由Java开发的开源软件项目，旨在提供一个开放易用的软件平台，使持续集成变成可能。<br />\n利用Jenkins持续集成Node.js项目之后，就不用每次都登录到服务器，执行pm2 restart xxx或者更原始一点的kill xx，然后node xxx。通过Jenkins，只需单击『立即构建』按钮，就可以自动从Git仓库拉取代码，然后部署到远程服务器，执行一些安装依赖包和测试的命令，最后启动应用。如果需要部署到多台服务器上，只需在Jenkins上多配置相应的服务器数量，就可以通过Jenkins部署到多台服务器上。\n\n## 通过Docker安装和启动Jenkins\nJenkins需要Java环境，有了Docker这个利器，我们就省去了安装Java环境的麻烦，只需执行如下命令即可。\n```\ndocker pull jenkins:latest\n```\n有一个通常的做法是要将Jenkins文件存储地址挂载到主机上，万一Jenkins的服务器重装或迁移，可以很方便的把之前的项目配置保留。所用可通过如下命令来启动Jenkins容器。\n```\ndocker run -d --name myjenkins -p 49001:8080 -v ${pwd}/data:/var/jenkins_home jenkins\n```\n上面安装和启动Jenkins容器的做法，常常会出现错误，错误日志如下。\n```\ntouch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied\nCan not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?\n```\n这里推荐直接从 https://github.com/denverdino/docker-jenkins 获得相关代码，并构建自己的Jenkins镜像。执行命令如下：\n```\ngit clone https://github.com/AliyunContainerService/docker-jenkins\ncd docker-jenkins/jenkins\ndocker build -t myjenkins .\n```\n然后基于新镜像启动Jenkins容器。\n```\ndocker rm -f jenkins\ndocker run -d -p 8080:8080 -p 50000:50000 -v $(pwd)/data:/var/jenkins_home --name jenkins myjenkins\n```\n用`docker-machine ip`获取到docker的IP后，用浏览器访问这个IP的8080端口，就会有如下图所示的Jenkins的主界面。\n![docker_homepage](http://nnblog-storage.b0.upaiyun.com/img/jenkins_homepage.jpg)\n\n## 通过Jenkins自动化部署Node.js项目\n要实现自动化部署，需要在Jenkins中安装Git Plugin和Publish Over SSH这连个插件。<br />\n插件安装完成后重启Jenkins，进入系统管理→系统设置来对插件进行简单的配置，增加远程的服务器配置。如下图所示，填入我们待部署的生产服务器的IP地址、SSH端口及用户名、密码等信息。如果远程服务是通过key来登录的，那么还需要把key的存放路径写上。\n![docker_jenkins_ssh](http://nnblog-storage.b0.upaiyun.com/img/jenkins_remotehost.jpg)\n回到Jenkins主页，单击左上角的『新建』按钮就可以开启一个新项目，给项目起名node_test，选择创建一个自由风格的软件项目，单击「OK」按钮，就进入了此项目的创建页面。<br />\n在配置页，我们找到『源码管理』选项，配置好Github上的源码地址。\n![docker_github_code](http://nnblog-storage.b0.upaiyun.com/img/jenkins_git.jpg)\n然后单击「Add」按钮，配置Github账号，Jenkins就是通过这个账号来拉取源代码的。\n![docker_github_account](http://nnblog-storage.b0.upaiyun.com/img/jenkins_git_account.png)\n在『构建』一栏，单击下拉菜单，选择Execute shell，Jenkins从Github获取代码代码是自动执行的，这一步主要是将最新的代码打包，如下图。\n![execShell](http://nnblog-storage.b0.upaiyun.com/img/execShell_1.png)\n将代码打包后需要把代码发送的远程的生产服务器上，这时需要选择「Send files...」选项，需要填入的指令如下图。\n![sendFile](http://nnblog-storage.b0.upaiyun.com/img/sendFile_1.png)\n在Source files一栏中将刚刚打包的文件名node_test.tar.gz填入<br />\n在Remote directory一栏中，填写发送代码包的远程保存地址，我们在这里写入var/，在远程主机上代码包的路径就是/var/node_test.tar.gz<br />\nExec Command一栏中的命令的主要作用是：\n1. 删除之前的容器。\n2. 删除原来项目的代码。\n3. 解压新代码。\n4. 在容器中安装新代码的依赖包，然后将真个应用启动起来。\n5. 删除发送过来的代码包。\n<br />\n\n至此项目配置完成，保存之后回到首页，单击左侧的『立即构建』按钮，如果小球变为蓝色的话就是构建成功，我们的项目也就部署成功了。\n\n*参考：https://yq.aliyun.com/articles/53990*、*《Node.js实战（第二季）》*\n","tags":["Jenkins"]},{"title":"初探Docker","url":"%2F2016%2F07%2F07%2F%E5%88%9D%E6%8E%A2Docker%2F","content":"![docker](http://nnblog-storage.b0.upaiyun.com/img/docker.jpg)\nDocker让开发人员易于快速构建可随时运行的容器化应用程序，它大大简化了管理和部署应用程序的任务。下面主要介绍一些基本指令和用法，由于主要是在Mac上使用，所以有些东西在Mac上才会涉及到。\n<!-- more -->\n## 安装（只适用于macOS）\n1. 下载官网的DockerToolbox.pkg安装包安装，这里推荐在阿里云上的镜像http://mirrors.aliyun.com/docker-toolbox/mac/?spm=0.0.0.0.8EK1Sh 。\n2. 使用brew安装\n```\nbrew cask update\nbrew cask install docker\n```\n\n## 快速开始\n```\ndocker-machine create --engine-registry-mirror=https://f9fd872q.mirror.aliyuncs.com -d virtualbox default\n# 创建一台安装有Docker环境的Linux虚拟机，指定机器名称为default，同时配置Docker加速器地址.\n```\n记得一定要用加速器，这很重要！\n```\ndocker-machine env default\neval \"$(docker-machine env default)\"\n```\n通过运行docker info指令，若能看到Containers、Running和Images等相关的信息，这就表明Docker环境搭建好了。\n\n## 一些指令\ndocker ps `列出所有运行中的容器` <br />\ndocker ps -a `列出所有容器` <br />\ndocker images `列出镜像` <br />\ndocker rmi [Image] `删除一个镜像` <br />\ndocker rm [Container] `删除一个容器` <br />\ndocker stop [Container] `停止一个正在运行的容器`\n\n**docker run [OPTIONS] IMAGE[:TAG] [COMMAND] [ARG...] 依附一个镜像运行一个容器**\n\nOPTIONS：\n- -d：分离模式，在后台运行容器，并且打印出容器ID\n- -i：保持输入流开放即使没有附加输入流\n- -t：分配一个伪造的终端输入\n- -p：匹配镜像内的网络端口号\n- -e：设置环境变量\n- -link：连接到另一个容器\n- -name：分配容器的名称，如果没有指定就会随机生成一个\n\nExamples：\n```\ndocker run -it ubuntu 启动一个依赖Ubuntu镜像并带有终端输入的容器\n\ndocker run --name db -d -e MYSQL_ROOT_PASSWORD=123 -p 3306:3306 mysql:latest 启动一个依赖mysql镜像的容器，-name db将容器命名为db；-d在后台运行；\n-e MYSQL_ROOT_PASSWORD=123设置环境变量，参数告诉docker所提供的环境变量，这之后跟着的变量正是MySQL镜像检查且用来设置的默认root用户的密码；\n-p 3306:3306告诉引擎用户想要将容器内的3306端口映射到外部的3306端口上。\n\ndocker exec -it db 告诉docker用户想要在名为db的容器里执行一个命令。\n\ndocker run -it -p 8123:8123 --link db:db -e DATABASE_HOST=db node4 使用link参数告诉docker我们想要连接到另外的容器上，link db:db连接\n到名为db的容器上，并且用db指代该容器。\n```\n## 在工程中使用Docker\n在项目工程中常常在根目录一个Dockerfile文件，用来构建项目的运行环境。下面是一个Node.js工程中Docerfile文件的示例。\n```\nFROM node:4  #指定基础镜像\n\nADD . /app  #将当前目录下的所有东西拷贝到名为app/的容器目录里\n\nRUN cd /app;\n\tnpm install --production  在镜像里运行命令\n\nEXPOSE  3000 #将服务的端口暴露出来\n\nCMD [\"node\"， \"/app/index.js\"]  #运行服务\n```\n有了这个Dockerfile文件后，只需在项目根目录执行docker build -t node4 .就可以在docker引擎中创建所需要的容器，并将代码放到容器中，-t node4表示使用标签node4标记该镜像，之后就可以使用这个标签来代指该镜像，.最后那个点表示在当前目录查找Dockerfile文件。<br />\n通过执行docker run -it -p 8123:8123 node4就可以将该容器运行起来了。\n## Docker Compose\n通常一个Node.js工程都需要数据库，我们将数据库也放在一个容器里面，那个这个Node.js工程所在的容器需要连到数据库所在的容器。我们可通过另一个Dockerfile创建一个数据库容器，并初始化一些数据。<br />\n这是创建数据库容器的Dockerfile\n```\nFROM mysql:5\n\nENV MYSQL_ROOT_PASSWORD 123  \nENV MYSQL_DATABASE users  \nENV MYSQL_USER users_service  \nENV MYSQL_PASSWORD 123\n\nADD setup.sql /docker-entrypoint-initdb.d #任何添加到镜像的 /docker-entrypoint-initdb.d目录的.sql或者.sh文件会在搭建DB的时候执行。\n```\ndocker build -t test-database .  <br />\ndocker run --name db test-database <br />\n这样需要通过docker run -it -p 8123:8123 --link db:db -e DATABASE_HOST=DB node4将node4容器连接到数据库容器，才能在Node.js工程中，访问数据库。<br />\n这样看似解决了问题，但如果Node.js工程还依赖Redis、RabbitMQ和MongoDb等，如果是手动来处理这些容器的连接，简直是费时费力，这时候就可以用docker-compose来构建一次构建多个容器间的连接。<br />\nDocker Compose需要一个docker-compose.yml文件，下面是一个示例：\n```\nversion: '2'  \nservices:  \n  users-service:\n    build: ./node4\n    ports:\n     - \"8123:8123\"\n    depends_on:\n     - db\n    environment:\n     - DATABASE_HOST=db\n  db:\n    build: ./test-database\n```\n通过执行`docker-compose build`指令，就可以构建docker-compose.yml文件里列出的每个镜像，通过`docker-compose up`将构建的镜像运行起来，`docker-compose down`停止运行的镜像。<br />\n在docker-compose.yml中，每个servies下的每个服务的build值告诉到哪里找到Dockerfile。<br />\n\n*参考：http://blog.jobbole.com/103069/*\n","tags":["Docker"]},{"title":"Effective JavaScript（一）","url":"%2F2016%2F05%2F14%2FEffective%20JavaScript%EF%BC%88%E4%B8%80%EF%BC%89%2F","content":"最近在读《Effective JavaScript》这本书，书中讲解了很多JS的语言细节和一些最佳实践，有一些是我平时使用时没有注意的，也有一些是自己踩过的坑，下面就列举一些我所留意的知识点。\n<!-- more -->\n## Number\nJS中的数字都是双精度的浮点数。JS中的整数仅仅是双精度浮点数的一个子集，而不是一个单独的数据类型。<br />\nJS的浮点数在运算的时候存在精度陷阱。\n```\n  0.1 + 0.2 // 0.30000000000000004\n```\n因为浮点数运算只能产生近似的结果，四舍五入到最接近的可表示的实数，当执行一系列的运算，随着舍入误差的积累，运算结果会越来越不精确。<br />\n若想要使*0.1+0.2*等于0.3，可以使用*Number((0.1+0.2).toFixed(1))* 来实现，注意toFixed()生成的是字符串，所以必须用Number来转换成数字。\n\nNaN是JS中的一个特殊数字，它是唯一一个不等于本身的数字。\n```\nvar a = Number(b); // a为NaN, 因为b未定义\na == a; // false\na != a; // true\n```\n\n## JS中的7个假值\nJS中只有7个布尔类型为false的值，它们分别是`false(Boolean)`、`0(Number)`、`-0(Nubmer)`、`\"\"(String)`、`null(Object)`、`NaN(Number)`、`undefined`。\n\n`注意，空对象{}和空数组[]它们的布尔类型为true。`\n\n## toString()和String()\n这两个方法都可以将一个值转换成一个字符串。\n#### toStirng()\n  - 大多数值都有toString()方法，null和undefined是没有的。\n  - 对应字符串型的值也可以使用toString()方法，它会返回该字符串的一个副本。\n  - toString()方法可以传递一个参数，表示数值的基数。（二进制，十进制）\n  ```\n  var t = 8;\n  t.toString(2); // '1000', 对于非数字类型设置toString()的参数是无效的。\n  ```\n\n#### String()\n任何值都可以使用String()方法。首先如果值有toString()方法，那么则使用该方法。其次，如果没有toString()方法，那就是null返回'null'，undefined返回'undefined'。\n\n## encodeURI和encodeURIComponent\nencodeURI和encodeURIComponent是把字符编码成UTF-8。<br />\nencodeURI方法不会对ASCII字母、数字、~!@#$%&*()=:/,;?+-_这些字符编码。<br />\nencodeURIComponent方法不会对ASCII字母、数字、~!*()'这些字符编码。<br />\n所以encodeURIComponent比encodeURI的编码范围更广。<br />\n它们对应的解码方法分别是decodeURI和decodeURIComponent。<br />\n\n## 闭包\n闭包存储的是外部变量的引用而不是值，并能读写这些值。<br />\n下面这段代码输出什么？\n```\nfunction wrapElements(a) {\n  var result = [], i, n;\n  for (i = 0; len = a.length; i < len; ++i) {\n    result[i] = function() { return a[i]; };  // ①\n  }\n  return result;\n}\n\nvar wrapped = wrapElements([10, 20, 30, 40, 50]);\nvar f = wrapped[0];\nf(); // ?\n```\n我刚开始以为是50，但实际是undefined。<br />\n因为闭包存储的是外部变量的引用，所以注释①那行代码中a[i]中的i其实是对i的引用，此时i的值为5，a[5]没有值，所以为undefined。\n","tags":["JavaScript"]},{"title":"在JavaScript中实现AOP","url":"%2F2016%2F01%2F17%2F%E5%9C%A8JavaScript%E4%B8%AD%E5%AE%9E%E7%8E%B0AOP%2F","content":"有一个公用的代码，可能在很多地方都会被用到，那么现在要做的就是，需要这个方法跑起来之前走一些东西，在这个方法跑完之后，还在处理一些东西。\n<!-- more -->\n## 概念解读\nAOP(Aspect Oriented Programming)，面向切面编程，主要实现的目的是针对业务处理过程中的切面进行提取，它所面对的是处理过程中的某个步骤或阶段，以获得逻辑过程中各部分之间低耦合性的隔离效果。\nAOP的主要思想是把一些与核心业务无关但又在多个模块使用的功能分离出来，然后动态给业务模块添加上 需要的功能。\nAOP可以实现对业务无侵入式地干扰。\n\n## 代码实战\n如果我们想得到某个函数的执行的开始时间和结束时间，可能会写出如下的代码：\n```\n    function test1(){\n        console.log(1);\n    }\n    console.log('test1 start:', Date.now());\n    test1();\n    console.log('test2 start:', Date.now());\n```\n上面的代码确实能完成响应的需求，但是扩展性不好，倘若遇到需要得到多个函数它们执行的开始和结束时间，如果还是按上面的那种方式，几乎同样的代码会写很多遍，如下：\n```\n    function test1(){\n        console.log(1);\n    }\n    function test2(){\n        console.log(2);\n    }\n    function test3(){\n        console.log(3);\n    }\n    ...\n    console.log('test1 start:', Date.now());\n    test1();\n    console.log('test1 start:', Date.now());\n\n    console.log('test2 start:', Date.now());\n    test2();\n    console.log('test2 start:', Date.now());\n\n    console.log('test3 start:', Date.now());\n    test3();\n    console.log('test3 start:', Date.now());\n    ...\n```\n像这样很不优雅，代码变得极难维护，这时候若实现AOP就能够很轻松的解决类似的问题。\n```\n    function test1(){\n        console.log(1);\n    }\n\n    Function.prototype.before = function(fn){\n        var self = this;  \n        fn(self.name); \n        self.apply(this, arguments);         \n    }\n    Function.prototype.after= function(fn){\n        var self = this;\n        self.apply(this, arguments);   \n        fn(self.name);\n    }\n    test1.before(printTime);\n    test1.after(printTime);\n\n    function printTime () {\n        console.log(arguments[0], 'end:', Date.now());\n    }    \n```\n每个函数都是Function类型的实例，因为Function通过prototype挂载了before方法和after方法，所以作为Function的实例，test1函数就拥有了before和after方法。现在执行test1.before(printTime)和test1.after(printTime)就会自动打印函数执行的开始和结束时间。\n但是，在上面的代码中发现test1执行了两次，可以用test1函数作为中转，通过执行before后将before的回调和before一起送到after去来解决这个问题。\n```\n    var funcName;\n    function test1(){\n        console.log(1);\n    }\n    Function.prototype.before = function(fn){\n        var self = this;\n        funcName = self.name;\n        return function(){\n            fn.apply(this, arguments);  \n            self.apply(self, arguments);\n        };\n    };\n\n    Function.prototype.after = function(fn){\n        var self = this;\n        return function(){\n            self.apply(self, arguments);\n            fn.apply(this, arguments);\n        };\n    };\n\n    test1.before(printTime).after(printTime)();  \n\n    function printTime () {\n        console.log(funcName, 'end:', Date.now());\n    } \n```\n在上面的代码中，test1函数调用before方法后得到一个匿名函数，该匿名函数继续调用其拥有的after方法，又得到一个匿名函数，最后执行这个匿名函数。最后这个匿名函数做的工作有打印test1()开始执行前的时间，执行test1函数，打印test1()执行后的时间。\n这样就通过before中的回调作为传递，test1就只执行一次。\n像最开始的需求那样，若有多个函数需要做相同的事，只需像下面这样就能做到。\n```\n    test1.before(printTime).after(printTime)();\n    test2.before(printTime).after(printTime)();\n    test3.before(printTime).after(printTime)();\n    .\n    .\n    .\n    testN.before(printTime).after(printTime)();\n```\n","tags":["AOP"]},{"title":"MongoDB中的mapReduce","url":"%2F2015%2F12%2F21%2FMongoDB%E4%B8%AD%E7%9A%84mapReduce%2F","content":"MongoDB中的mapReduce一般用于处理大数据集。\n<!-- more -->\n## 基本用法\n以下是基本的mapReduce命令的语法：\n```\n>db.collection.mapReduce(\n   function() {emit(key,value);},  //map function\n   function(key,values) {return reduceFunction},   //reduce function\n   {\n      out: collection,\n      query: document,\n      sort: document,\n      limit: number\n   }\n)\n```\n使用mapReduce要实现连个函数Map函数和Reduce函数。\nMap函数调用emit(key, value)，遍历collection中所有的记录，表示集合会按照指定的key进行映射分组，类似group by，分组的结果为value。\nReduce函数将对Map函数传递的key与value进行处理。\n参数说明：\n- **map:** 映射函数（生成键值对序列，作为reduce函数参数）。\n- **reduce:** 统计函数，reduce函数的任务就是将key-values编程key-value，也就是把values数组变成一个单一的值value。\n- **out:** 统计结果存放集合，不指定则使用临时集合，在客户端断开后自动删除。\n- **query:** 一个筛选条件，只有满足条件的文档才会调用Map函数。（query，limit，sort可随意组合）\n- **sort:** 排序参数，也就是在发往Map函数前给集合排序，可以优化分组机制。\n- **limit:** 发往Map函数的文档数量的上限，要是没有limit，单独使用sort的用处不大。\n\n## 演示\n插入数据：\n```\n> db.books.find()\n{ \"_id\" : ObjectId(\"533ee1e8634249165a819cd0\"), \"name\" : \"apue\", \"pagenum\" : 1023 }\n{ \"_id\" : ObjectId(\"533ee273634249165a819cd1\"), \"name\" : \"clrs\", \"pagenum\" : 2000 }\n{ \"_id\" : ObjectId(\"533ee2ab634249165a819cd2\"), \"name\" : \"python book\", \"pagenum\" : 600 }\n{ \"_id\" : ObjectId(\"533ee2b7634249165a819cd3\"), \"name\" : \"golang book\", \"pagenum\" : 400 }\n{ \"_id\" : ObjectId(\"533ee2ca634249165a819cd4\"), \"name\" : \"linux book\", \"pagenum\" : 1500 }\n```\n\nMap函数：\n```\nvar map = function(){\n    var category;\n    if ( this.pageNum >= 1000 ){\n        category = 'Big Books';\n    }else{\n        category = 'Small Books';\n    }\n    emit(category, {name: this.name});\n}\n```\n\nMap函数里面会调用emit(key, value)，集合会按照指定的key进行映射分组, 类似group by。this指向每一条被迭代的数据。\n\n上面执行Map函数后的结果为: (按照category分组, 分组结果是{name: this.name}的list)\n```\n{\"big books\",[{name: \"apue\"}, {name : \"linux book\"}, {name : \"clrs\"}]]);\n{\"small books\",[{name: \"python book\"}, {name : \"golang book\"}]);\n```\n\nReduce函数：\n```\nvar reduce = function(key, values) {\n    var sum = 0;\n    values.forEach(function(doc) {\n    sum += 1;\n    });\n    return {books: sum};\n};\n```\nreduce函数会对Map分组后的数据进行分组简化，注意：在reduce(key,value)中的key就是emit中的key，value为emit分组后的emit(value)的集合, 这里是{name: this.name}的list。\n\nmapReduce函数：\n```\n> var count  = db.books.mapReduce(map, reduce, {out: \"book_results\"});\n> db[count.result].find()\n{ \"_id\" : \"big books\", \"value\" : { \"books\" : 3 } }\n{ \"_id\" : \"small books\", \"value\" : { \"books\" : 2 } }\n```\n\n```\n> db.books.mapReduce(map, reduce, {out: \"book_results\"});\n{\n        \"result\" : \"book_results\",\n        \"timeMillis\" : 107,\n        \"counts\" : {\n                \"input\" : 5,\n                \"emit\" : 5,\n                \"reduce\" : 2,\n                \"output\" : 2\n        },\n        \"ok\" : 1,\n}\n```\n\n具体参数说明：\n- **result:** 存储结果的collection的名字，这是个临时集合，mapReduce的连接关闭后制动就被删除了。\n- **timeMillis:** 执行花费的时间，毫秒为单位。\n- **input:** 满足条件被发送到Map函数的个数。\n- **emit:** 在Map函数中emit被调用的次数，也就是所有集合中的数据总量。\n- **output:** 结果集合中的个数。\n\n查询结果集中的数据：\n```\n> db.book_results.find()\n{ \"_id\" : \"big books\", \"value\" : { \"books\" : 3 } }\n{ \"_id\" : \"small books\", \"value\" : { \"books\" : 2 } }\n```\n","tags":["MongoDB"]},{"title":"yield和yield*","url":"%2F2015%2F12%2F18%2Fyield%E5%92%8Cyield%2F","content":"在ES6的generator函数中经常能见到yield和yield*，开始不是很清楚它们之间有什么区别，在参考了一些资料过后渐渐有一些清楚了。\n<!-- more -->\n## Array与String\n```\nfunction* GenFunc(){\n    yield [1, 2];\n    yield* [3, 4];\n    yield \"56\";\n    yield* \"78\";\n}\n\nvar gen = GenFunc();\nconsole.log(gen.next().value);  //[1, 2]\nconsole.log(gen.next().value);  //3\nconsole.log(gen.next().value);  //4\nconsole.log(gen.next().value);  //\"56\"\nconsole.log(gen.next().value);  //7\nconsole.log(gen.next().value);  //8\n```\n从上面的代码可以看出，yield*后面如果跟的是一个Array或String，每执行一次next()函数，将会迭代Array或String中的一个元素，而如果是普通的yield则直接返回这个对象。\n\n## arguments\n```\nfunction* GenFunc(){\n    yield arguments;\n    yield* arguments;\n}\n\nvar gen = GenFunc(1, 2);\nconsole.log(gen.next().value);  //{ '0': 1, '1': 2}\nconsole.log(gen.next().value);  //1\nconsole.log(gen.next().value);  //2\n```\narguments的情况和Array或String情况一样，说明yield*后面如果跟的是一个可迭代对象，每执行一次next()函数，将会迭代一次这个对象。\n\n## Generator\n```\nfunction* Gen1(){\n    yield 2;\n    yield 3;\n    return 'liuxin';\n}\nfunction* Gen2(){\n    yield 1;\n    var a = yield* Gen1();\n    console.log(a);   //打印'liuxin'\n    yield 4;\n}\n\n\nvar gen2 = Gen2();\nconsole.log(gen2.next().value);  //1\nconsole.log(gen2.next().value);  //2\nconsole.log(gen2.next().value);  //3\nconsole.log(gen2.next().value);  //4\n```\nyield*后面如果跟的是一个Generator函数，那么便会执行这个Generator函数，同时yield*这个表达式的值就是这个Generator函数的返回值。\n\n## Object\n```\nfunction* GenFunc(){\n    yield {a: '1', b: '2'};\n    yield* {a: '1', b: '2'};\n}\n\nvar gen = GenFunc();\nconsole.log(gen.next().value);  //{ a: '1', b: '2'}\nconsole.log(gen.next().value);  //1\nconsole.log(gen.next().value);  //2\n```\n情况和arguments的情况一样。\n\n## 总结\n根据上面的所有代码可以知道，yield*后面接受一个iterable object作为参数，然后去迭代(iterate)这个迭代器（iterable object)，同时yield*本身这个表达式的值就是迭代器迭代完成时的返回值，以及yield*可以用来在一个generator函数里“执行”另一个generator函数，并可取得其返回值。\n\n参考：http://taobaofed.org/blog/2015/11/19/yield-and-delegating-yield/\n\n\n\n","tags":["Node.js"]},{"title":"SQL成长记","url":"%2F2015%2F11%2F19%2FSQL%E6%88%90%E9%95%BF%E8%AE%B0%2F","content":"这一段时间在进行服务器端的开发，在开发的过程需要遍写大量的SQL语句来操作数据库，对于之前SQL还是菜鸟水平的我来说在开发的过程中遇到很多之前不太熟悉的地方，有些业务需要的SQL较为复杂，需要连接好几张表，数据呈现的要求也多种多样，最后也还算顺利的完成了业务。\n<!-- more -->\n下面是一些在这个过程中让我对SQL有更深入认识的SQL语句：\n\n## 模糊查询并按匹配度进行排序\n\n```\n    select name from user where name like '%foo%' order by abs(length(name)-length('foo'));\n```\n\n这是一种按匹配度排序很简陋的做法，在很多情况下都存在问题，如果是对查询的结果按匹配度排序有着重度的需求，最好不要使用这个。\n\n## 分组统计数量\n\n```\n    select count(1) 'count',class from user where gender='male' group by class;\n```\n\n上面的SQL语句是在user表中查询每个班男生的数量，通过group by class做到了按班分组，通过count(1)做到了统计数量。\n\n## 随机查询指定数量的数据\n\n```\n    select id,name from user where type=5 order by rand() limit 10;\n```\n\n在这里用到了SQL的内置函数rand()用于获取随机数据，不过处于性能考虑，在SQL语句中应尽量少用内置函数。\n\n## 在查询字段中指定一个常量\n\n```\n    select k.* from (select id,create_date,content,'homework' as 'type' from comments where isHomework=1 union select id,create_date,content,'post' as 'type' from posts) k where k.account_id='123456' order by k.create_date desc;\n```\n\n上面的SQL语句在查询comments表和posts表是分别在查询字段中增加了常量homework和post，并指定了列名为type，也就是在临时表k中，存在名为type的列，它下面的值要么是homework，要么是post。\n\n","tags":["SQL"]},{"title":"在Redis中进行分页排序查询","url":"%2F2015%2F11%2F17%2F%E5%9C%A8Redis%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%88%86%E9%A1%B5%E6%8E%92%E5%BA%8F%E6%9F%A5%E8%AF%A2%2F","content":"Redis是一个高效的内存数据库，它支持包括String、List、Set、SortedSet和Hash等数据类型的存储，在Redis中通常根据数据的key查询其value值，Redis没有条件查询，在面对一些需要分页或排序的场景时（如评论，时间线），Redis就不太好不处理了。\n<!-- more -->\n前段时间在项目中需要将每个主题下的用户的评论组装好写入Redis中，每个主题会有一个topicId，每一条评论会和topicId关联起来，得到大致的数据模型如下：\n\n```\n{\n    topicId: 'xxxxxxxx',\n    comments: [\n        {\n            username: 'niuniu',\n            createDate: 1447747334791,\n            content: '在Redis中分页',\n            commentId: 'xxxxxxx',\n            reply: [\n                {\n                    content: 'yyyyyy'\n                    username: 'niuniu'\n                },\n                ...\n            ]\n        },\n        ...\n    ]\n}\n     \n```\n\n将评论数据从MySQL查询出来组装好存到Redis后，以后每次就可以从Redis获取组装好的评论数据，从上面的数据模型可以看出数据都是key-value型数据，无疑要采用hash进行存储，但是每次拿取评论数据时需要分页而且还要按createDate字段进行排序，hash肯定是不能做到分页和排序的。\n\n那么，就挨个看一下Redis所支持的数据类型：\n> **String: **主要用于存储字符串，显然不支持分页和排序。\n> **Hash: **主要用于存储key-value型数据，评论模型中全是key-value型数据，所以在这里Hash无疑会用到。\n> **List: **主要用于存储一个列表，列表中的每一个元素按元素的插入时的顺序进行保存，如果我们将评论模型按createDate排好序后再插入List中，似乎就能做到排序了，而且再利用List中的LRANGE key start stop指令还能做到分页。嗯，到这里List似乎满足了我们分页和排序的要求，但是评论还会被删除，就需要更新Redis中的数据，如果每次删除评论后都将Redis中的数据全部重新写入一次，显然不够优雅，效率也会大打折扣，如果能删除指定的数据无疑会更好，而List中涉及到删除数据的就只有LPOP和RPOP这两条指令，但LPOP和RPOP只能删除列表头和列表尾的数据，不能删除指定位置的数据，所以List也不太适合。\n> **Set: **主要存储无序集合，无序！排除。\n> **SortedSet: **主要存储有序集合，SortedSet的添加元素指令*ZADD key score member [[score,member]...]*会给每个添加的元素member绑定一个用于排序的值score，SortedSet就会根据score值的大小对元素进行排序，在这里就可以将createDate当作score用于排序，SortedSet中的指令*ZREVRANGE key start stop*又可以返回指定区间内的成员，可以用来做分页，SortedSet的指令ZREM key member可以根据key移除指定的成员，能满足删评论的要求，所以，SortedSet在这里是最适合的。\n\n所以，我需要用到的数据类型有SortSet和Hash，SortSet用于做分页排序，Hash用于存储具体的键值对数据，我画出了如下的结构图：\n\n![结构图](http://nnblog-storage.b0.upaiyun.com/img/redis.png!watermark1.0)\n\n在上图的SortSet结构中将每个主题的topicId作为set的key，将与该主题关联的评论的createDate和commentId分别作为set的score和member，commentId的顺序就根据createDate的大小进行排列。\n当需要查询某个主题某一页的评论时，就可主题的topicId通过指令*zrevrange topicId (page-1)×10 (page-1)×10+perPage*这样就能找出某个主题下某一页的按时间排好顺序的所有评论的commintId。page为查询第几页的页码，perPage为每页显示的条数。\n当找到所有评论的commentId后，就可以把这些commentId作为key去Hash结构中去查询该条评论对应的内容。\n这样就利用SortSet和Hash两种结构在Redis中达到了分页和排序的目的。\n","tags":["Redis"]},{"title":"JavaScript中的this","url":"%2F2015%2F11%2F16%2FJavaScript%E4%B8%AD%E7%9A%84this%2F","content":"\nJavaScript中的this在运行期进行绑定，这使得JavaScript中的this关键字具备多重含义。\n<!-- more -->\nJavaScript中的this到底指向什么？可以用下面一张图来解释：\n\n![this的指向](http://nnblog-storage.b0.upaiyun.com/img/this.jpg!watermark1.0)\n\n## 指向全局对象的例子\n\n```\nvar point = {\n\tx: 0,\n\ty: 0,\n\tmoveTo: function(x, y){\n\t\t//内部函数\n\t\tvar moveX = function(x){\n\t\t\tthis.x = x;   //this指向什么？window或global\n\t\t}；\n\t\t//内部函数\n\t\tvar moveY = function(y){\n\t\t\tthis.y = y;    //this指向什么？window或global\n\t\t}；\n\t\tmoveX(x);\n\t\tmoveY(y);\n\t}\n};\npoint.moveTo(1, 1);\npoint.x;  //=>0\npoint.y;  //=>0\nx;  //=>1\ny;   //=>1\n```\n在上面的代码中，moveX(x)函数调用既不是用new进行调用，也不是用dot(.)进行调用，所以this指向window。\n\n## 构造函数的例子\n\n```\nfunction Point(x, y){\n\tthis.x = x; // this?\n\tthis.y = y; // this?\n}\nvar np = new Point(1, 1);\nnp.x; //1\nvar p = Point(2, 2);\np.x;  //error, p是一个空对象undefined\nwindow.x;  //2\n```\n\n上面的代码中var np = new Point(1, 1)是用new进行调用，所以this指向创建的对象np，所以np.x就为1。\nvar p = Point(2, 2)函数调用既不是用new进行调用，也不是用dot(.)进行调用，所以this指向window。               \n\n## call和apply进行调用的例子\n\n```\nfunction Point(x, y){\n\tthis.x = x;\n\tthis.y = y;\n\tthis.moveTo = function(x, y){\n\t\tthis.x = x;\n\t\tthis.y = y;\n\t\t}；\n}\nvar p1 = new Point(0, 0);\nvar p2 = {x: 0, y: 0};\np1.moveTo.apply(p2, [10, 10]);  //apply实际上为p2.moveTo(10, 10)\np2.x   //10\n```\n\napply和call这两个方法可以改变函数执行的上下文，即改变this绑定的对象。p1.moveTo.apply(p2, [10,10])实际上是p2.moveTo(10, 10)。那么p2.moveTo(10, 10)可解释为不是new调用也而是dot(.)调用，所以this绑定的对象是p2。\n","tags":["this"]},{"title":" 这个博客怎么搭建的？","url":"%2F2015%2F11%2F15%2F%E8%BF%99%E4%B8%AA%E5%8D%9A%E5%AE%A2%E6%80%8E%E4%B9%88%E6%90%AD%E5%BB%BA%E7%9A%84%EF%BC%9F%2F","content":"本站是一个静态的个人博客网站，托管在[Github Pages](https://pages.github.com)上面。\n<!-- more -->\n## Github Pages\n\n要想使用Github Pages服务，则需要在[Github](https://github.com)上创建一个与自己用户名有关的仓库（当然，得先有Github账号），如用户名若为niuniu，则创建的仓库名称应为niuniu.github.io。\n\n## GoDaddy\n\nliu-xin.me这个域名是在[GoDaddy](https://www.godaddy.com/)上申请的。申请完后如果想要通过自己的域名访问在Github上创建的仓库，则需要配置一下域名的DNS，我的配置如下图：\n\n![域名的DNS配置](http://nnblog-storage.b0.upaiyun.com/img/godaddyDNS.png)\n\n当然到这里还不够，还需要在Github仓库里创建一个名为CNAME的文件，CNAME文件里的内容为申请的域名，我的CNAME文件里的内容就为liu-xin.me。\n\n除此之外，仓库里面还需要一个index.html文件，在index.html中通过HTML语言编辑好自己想要的内容后，然后访问自己的域名就可以看到自己编辑的东西了。\n\n## Hexo\n\n本站采用[Hexo](https://hexo.io)，这里不对Hexo作详细的介绍，需要注意的是当把自己的博客部署到Github上时，需要把CNAME文件放到通过Hexo生成的工程的source目录下，如果需要README.md文件也需要将该文件放在source目录下，并在工程的_config.yml文件中增加skip_render: README.md的配置。\n","tags":["Hexo"]}]